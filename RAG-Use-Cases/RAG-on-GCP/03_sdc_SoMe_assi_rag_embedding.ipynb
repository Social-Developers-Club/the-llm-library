{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4f4884-2f08-49b4-8d96-4d2024b31244",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SDC SocialMedia Assist RAG + Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e205327-7a41-4825-9f9b-87f71a1ade7d",
   "metadata": {},
   "source": [
    "## Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b526c50a-963b-4a27-988a-9ea5a58e0937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.globals import set_debug\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_community import BigQueryVectorStore, VertexFSVectorStore,GCSFileLoader\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\"the-llm-library/RAG-Use-Cases/RAG-on-GCP/config/config.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36563b6d-d58b-4d21-a106-b6dcd6c55610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = config[\"PROJECT_ID\"]\n",
    "LOCATION = config[\"LOCATION\"]\n",
    "# CloudStorage\n",
    "BUCKET_NAME = config[\"BUCKET_NAME\"]\n",
    "BLOB_NAME = config[\"BLOB_NAME\"]\n",
    "\n",
    "# BigQuery\n",
    "DATASET_ID = config[\"DATASET_ID\"]\n",
    "TABLE_ID = config[\"TABLE_ID\"]\n",
    "\n",
    "EMBEDDING_MODEL = config[\"EMBEDDING_MODEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4cf8f3-8d8d-49f1-9a94-774b9495d826",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b2641e-c21d-4961-a919-b22cf73843b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_embedding_model(embedding_model, project_id):\n",
    "    \n",
    "    embedding_model = VertexAIEmbeddings(\n",
    "        model_name=embedding_model, project=project_id\n",
    "    )\n",
    "    \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d642010-d40e-4ef3-bd65-282046977f0c",
   "metadata": {},
   "source": [
    "## DataPrep PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8834a24-e6a4-4cab-8926-877ec43f89e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Cloud Storage client\n",
    "\n",
    "loader = GCSFileLoader(\n",
    "    project_name=PROJECT_ID, bucket=BUCKET_NAME, blob=BLOB_NAME\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c349f0d-c639-4656-b416-0882b94e0e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 2\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6503932-af3e-4799-8165-2952af7889c7",
   "metadata": {},
   "source": [
    "## Configure BigQueryVectorStore as Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361cae19-8013-4621-9eb9-67e2644e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = build_embedding_model(EMBEDDING_MODEL, PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e43343-ddbc-44d6-9033-f29ced57005f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings_table():\n",
    "    dataset_id = 'sdc_marketing' # has to be created in bQ in beforehand\n",
    "    table_id = 'sdc_instagram_guideline'\n",
    "\n",
    "    schema = [\n",
    "        bigquery.SchemaField('document_id', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('text', 'STRING', mode='REQUIRED'),\n",
    "        bigquery.SchemaField('embedding', 'FLOAT64', mode='REPEATED'),\n",
    "    ]\n",
    "\n",
    "    table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = bigquery_client.create_table(table, exists_ok=True)\n",
    "    print(f\"Created table {table.full_table_id}\")\n",
    "\n",
    "# create_embeddings_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cba0ffe-ff2f-45ae-a19d-a24462b266dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery table sdc-gen-ai.sdc_marketing.sdc_instagram_guideline initialized/validated as persistent storage. Access via BigQuery console:\n",
      " https://console.cloud.google.com/bigquery?project=sdc-gen-ai&ws=!1m5!1m4!4m3!1ssdc-gen-ai!2ssdc_marketing!3ssdc_instagram_guideline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    dataset_name=DATASET_ID,\n",
    "    table_name=TABLE_ID,\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed366ca4-fdd3-4832-bcb8-270659f6eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7816d97-fc35-432a-a95a-6dc7360922cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# bq_store.similarity_search(\n",
    "#     \"Welchen Zweck hat das Posting?\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6c7c23-6666-477d-b81c-e876c0679831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_retriever = bq_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20655d8f-5a66-492f-98f2-fff23a4793b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_88365/22626390.py:36: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the agent (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "def build_system_prompt():\n",
    "    \n",
    "    # Define your custom prompt\n",
    "    custom_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    "        template=\"\"\"\n",
    "        Du bist ein hilfsbereiter KI-Assistent für unser Unternehmen, spezialisiert auf Social-Media-Strategien und die Einarbeitung neuer Teammitglieder. \n",
    "        Nutze das vorhandene Wissen aus dem Kontext, um präzise und nützliche Antworten zu liefern.\n",
    "\n",
    "        Gesprächsverlauf:\n",
    "        {chat_history}\n",
    "\n",
    "        Kontext:\n",
    "        {context}\n",
    "\n",
    "        Frage:\n",
    "        {question}\n",
    "\n",
    "        Bitte beachte bei deiner Antwort:\n",
    "        - Gehe spezifisch auf die Bedürfnisse des Nutzers ein.\n",
    "        - Nur auf Nachfrage! Biete praktische Tipps für Social-Media-Posts im Unternehmenskontext.\n",
    "        - Unterstütze neue Kollegen mit klaren Anweisungen und Ressourcen.\n",
    "        - Verwende eine positive und motivierende Sprache.\n",
    "        - Halte dich kurze und beantworte im ersten schritt erst mal nur die Nutzer-Frage\n",
    "\n",
    "        Antwort:\n",
    "        \"\"\"\n",
    "        )\n",
    "    return custom_prompt\n",
    "custom_prompt = build_system_prompt()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = VertexAI(model_name=\"gemini-1.5-flash-002\")\n",
    "\n",
    "# Set up the conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up the ConversationalRetrievalChain with the custom prompt\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=langchain_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "# Function to interact with the agent\n",
    "def chat_with_agent():\n",
    "    print(\"Start chatting with the agent (type 'exit' to stop):\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = conversational_chain.invoke({\"question\": user_input})\n",
    "        print(f\"\\nAssistant: {response['answer']}\")\n",
    "\n",
    "# Start the chat\n",
    "chat_with_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf3d9f-1314-46f2-956c-1bc6566e34a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
